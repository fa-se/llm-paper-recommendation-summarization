{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T15:35:13.856086Z",
     "start_time": "2024-08-31T15:35:13.847816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.join(project_path, '.env'), override=True)"
   ],
   "id": "5d052ad074041e4c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T15:35:13.951073Z",
     "start_time": "2024-08-31T15:35:13.891698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pyalex\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "# monkey patch pyalex.Work to implement __hash__ (using id)\n",
    "def custom_hash(self):\n",
    "    return id(self)\n",
    "\n",
    "\n",
    "pyalex.Work.__hash__ = custom_hash"
   ],
   "id": "f6925a3a16057d14",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T15:35:14.904857Z",
     "start_time": "2024-08-31T15:35:14.051618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core import retrieval, publication_repository\n",
    "from core.dataclasses.data_classes import Work"
   ],
   "id": "94e083a129bf7c54",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T15:35:14.914913Z",
     "start_time": "2024-08-31T15:35:14.913377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_publication_table():\n",
    "    publication_repository.truncate()"
   ],
   "id": "f51bcef867db38d5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T15:35:14.996675Z",
     "start_time": "2024-08-31T15:35:14.988789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_publications_from_ids(ids: list[str]) -> list[pyalex.Work]:\n",
    "    publications = []\n",
    "    for work_id in ids:\n",
    "        work = pyalex.Works()[work_id]\n",
    "        publications.append(work)\n",
    "    return publications\n",
    "\n",
    "\n",
    "def get_random_publications_openalex(n: int) -> list[pyalex.Work]:\n",
    "    pager = (pyalex.Works().sample(n, seed=42)\n",
    "             .filter(has_abstract=True)\n",
    "             .filter(has_references=True)\n",
    "             .paginate(method=\"page\", per_page=100))\n",
    "    publications = []\n",
    "    for page in pager:\n",
    "        publications.extend(page)\n",
    "    return publications\n",
    "\n",
    "\n",
    "def filter_ineligible_publications(publications: list[pyalex.Work], max: int) -> list[pyalex.Work]:\n",
    "    filtered_publications = []\n",
    "    for publication in publications:\n",
    "        if len(publication[\"referenced_works\"]) >= 20:\n",
    "            filtered_publications.append(publication)\n",
    "            if len(filtered_publications) >= max:\n",
    "                break\n",
    "\n",
    "    return filtered_publications\n",
    "\n",
    "\n",
    "def get_referenced_publications(publication: pyalex.Work, require_abstract=False) -> set[pyalex.Work]:\n",
    "    referenced_publications = set()\n",
    "    reference_ids = set(publication[\"referenced_works\"])\n",
    "    reference_ids = [\"W\" + reference.split(\"W\")[-1] for reference in\n",
    "                     reference_ids]  # use shortened ids because query gets too long otherwise \n",
    "    for i in range(0, len(reference_ids), 100):\n",
    "        query = (\n",
    "            pyalex.Works()\n",
    "            .filter(openalex=\"|\".join(reference_ids[i: i + 100]))\n",
    "        )\n",
    "        if require_abstract:\n",
    "            query = query.filter(has_abstract=True)\n",
    "\n",
    "        for pyalex_work in chain(*query.paginate(per_page=100, n_max=100)):\n",
    "            if pyalex_work[\n",
    "                \"id\"] != \"https://openalex.org/W4285719527\":  # deleted works are represented by this dummy entity\n",
    "                referenced_publications.add(pyalex_work)\n",
    "\n",
    "    return referenced_publications\n",
    "\n",
    "\n",
    "def convert_to_pyalex_works(works: list[Work]) -> list[pyalex.Work]:\n",
    "    pyalex_works = []\n",
    "    openalex_ids = [f\"W{work.id}\" for work in works]\n",
    "\n",
    "    for i in range(0, len(openalex_ids), 100):\n",
    "        query = pyalex.Works().filter(openalex=\"|\".join(openalex_ids[i: i + 100]))\n",
    "        for pyalex_work in chain(*query.paginate(per_page=100, n_max=100)):\n",
    "            pyalex_works.append(pyalex_work)\n",
    "\n",
    "    return pyalex_works\n",
    "\n",
    "\n",
    "def initial_retrieval(query, start_date, limit, num_topics):\n",
    "    topics, works = retrieval.initialize_for_query(query, start_date, limit, num_topics)\n",
    "    return topics, works\n",
    "\n",
    "\n",
    "def insert_publications(publications: list[pyalex.Work]):\n",
    "    from datetime import datetime\n",
    "    from core.llm_interfaces import OpenAIInterface\n",
    "    # first, convert to our own dataclass \n",
    "    publications_converted = [Work(publication) for publication in publications]\n",
    "\n",
    "    openai_interface = OpenAIInterface(print_usage_info=False)\n",
    "    abstracts = [work.abstract for work in publications_converted]\n",
    "    abstract_embeddings = openai_interface.create_embedding_batch(abstracts)\n",
    "\n",
    "    # make sure all referenced works are in the database\n",
    "    for i, work in enumerate(publications_converted):\n",
    "        publication = publication_repository.get_by_openalex_id(work.id)\n",
    "        if not publication:\n",
    "            publication_repository.create(\n",
    "                openalex_id=work.id,\n",
    "                title=work.title,\n",
    "                authors=work.authors,\n",
    "                abstract=work.abstract,\n",
    "                published=work.publication_date,\n",
    "                accessed=datetime.now(),\n",
    "                embedding=abstract_embeddings[i],\n",
    "            )\n",
    "    print(\"Rebuilding BM25 index...\")\n",
    "    publication_repository.rebuild_bm25()\n",
    "    print(\"Rebuilt BM25 index.\")\n",
    "\n",
    "\n",
    "def check_all_publications_in_db(publications: list[pyalex.Work]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Checks whether all cited works are actually in the database, and returns a list of those that are not.\n",
    "    \"\"\"\n",
    "    missed_publications = []\n",
    "    for publication in publications:\n",
    "        result = publication_repository.get_by_openalex_id(int(publication[\"id\"].split(\"W\")[-1]))\n",
    "        if not publication:\n",
    "            missed_publications.append(result)\n",
    "    return missed_publications\n",
    "\n",
    "\n",
    "def get_retrieval_results(publication: pyalex.Work, n: int = 100) -> list[Work]:\n",
    "    from core.services.publication_service import SearchType\n",
    "    start_date = datetime(1900, 1, 1)\n",
    "    query = publication[\"abstract\"]\n",
    "    results_hybrid = retrieval.get_relevant_works_for_query(query, n, start_date, SearchType.HYBRID,\n",
    "                                                            rerank=False)  # some buffer to account for possible duplicates\n",
    "\n",
    "    return results_hybrid[:n]\n",
    "\n",
    "\n",
    "def rerank(query_work: pyalex.Work, retrieval_results: list[Work]) -> list[Work]:\n",
    "    return retrieval._rerank(query_work[\"abstract\"], retrieval_results)\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ResultStats:\n",
    "    run: int\n",
    "    type: str\n",
    "    query_work: str\n",
    "    result_work: str\n",
    "    result_rank: int\n",
    "    common_references: int\n",
    "    max_common_references: int\n",
    "    is_reference_of_citing_work: bool\n",
    "    num_query_work_references_in_corpus: int\n",
    "\n",
    "\n",
    "def calculate_stats(query_work: pyalex.Work, query_work_references_in_corpus: list[pyalex.Work],\n",
    "                    retrieval_results: list[pyalex.Work], run: int, type: str) -> list[ResultStats]:\n",
    "    stats = []\n",
    "\n",
    "    query_work_reference_ids = set(query_work[\"referenced_works\"])\n",
    "    num_query_work_references_in_corpus = len(\n",
    "        query_work_references_in_corpus)  # relevant for calculating (retrieved_references / max_retrieved_references)\n",
    "\n",
    "    for i, retrieved_work in enumerate(retrieval_results):\n",
    "        result_reference_ids = set(retrieved_work[\"referenced_works\"])\n",
    "        common_references = query_work_reference_ids.intersection(result_reference_ids)\n",
    "        max_common_references = min(len(query_work_reference_ids), len(result_reference_ids))\n",
    "        is_reference_of_citing_work = retrieved_work[\"id\"] in query_work_reference_ids\n",
    "\n",
    "        stats.append(\n",
    "            ResultStats(\n",
    "                run=run,\n",
    "                type=type,\n",
    "                query_work=query_work[\"id\"],\n",
    "                result_work=retrieved_work[\"id\"],\n",
    "                result_rank=i + 1,  # 1-indexed\n",
    "                common_references=len(common_references),\n",
    "                max_common_references=max_common_references,\n",
    "                is_reference_of_citing_work=is_reference_of_citing_work,\n",
    "                num_query_work_references_in_corpus=num_query_work_references_in_corpus\n",
    "            ))\n",
    "\n",
    "    return stats"
   ],
   "id": "4b45ab2381a8f3a4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T15:44:10.812518Z",
     "start_time": "2024-08-31T15:35:15.059049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "timestamp = datetime.now().isoformat()\n",
    "\n",
    "n = 3\n",
    "limit_initial_retrieval = 2000\n",
    "start_date = datetime(2020, 1, 1)\n",
    "random_publications = get_random_publications_openalex(n * 5)\n",
    "filtered_publications = filter_ineligible_publications(random_publications, max=n)\n",
    "\n",
    "stats = []\n",
    "for i, query_work in enumerate(filtered_publications):\n",
    "    print(f\"Run: {i} --- Title: {query_work['title']}, id: {query_work['id']}\")\n",
    "    clean_publication_table()\n",
    "\n",
    "    topics, initial_retrieval_works = initial_retrieval(query_work[\"abstract\"], start_date, limit_initial_retrieval, 10)\n",
    "    print(f\"Top 3 topics: {topics[:3]}\")\n",
    "    print(f\"Retrieved and inserted {len(initial_retrieval_works)} publications.\")\n",
    "    referenced_works = get_referenced_publications(query_work, require_abstract=True)\n",
    "    insert_publications(referenced_works)\n",
    "    print(f\"Inserted {len(referenced_works)} referenced works.\")\n",
    "    # sanity check: are all referenced works in the database?\n",
    "    print(f\"Found all referenced works in the database? {len(check_all_publications_in_db(referenced_works)) == 0}\")\n",
    "    retrieval_results = get_retrieval_results(query_work, 100)\n",
    "    print(f\"Top 5 results: {\"\".join(f'\\n\\t{result.title}' for result in retrieval_results[:5])}\")\n",
    "    print(f\"Reranking...\")\n",
    "    reranked_results = rerank(query_work, retrieval_results)\n",
    "    print(f\"Reranked results: {\"\".join(f'\\n\\t{result.title}' for result in reranked_results[:5])}\")\n",
    "\n",
    "    # get corresponding pyalex.Work objects so we can access reference information\n",
    "    retrieval_results_pyalex = convert_to_pyalex_works(retrieval_results)\n",
    "    retrieval_stats = calculate_stats(query_work=query_work, query_work_references_in_corpus=referenced_works,\n",
    "                                      retrieval_results=retrieval_results_pyalex, run=i, type=\"retrieval\")\n",
    "    reranking_stats = calculate_stats(query_work=query_work, query_work_references_in_corpus=referenced_works,\n",
    "                                      retrieval_results=retrieval_results_pyalex, run=i, type=\"reranking\")\n",
    "\n",
    "    stats.extend(retrieval_stats)\n",
    "    stats.extend(reranking_stats)\n",
    "\n",
    "    df = pd.DataFrame(stats)\n",
    "    # save to timestamped pickle file\n",
    "    df.to_pickle(f\"eval_reranking_{timestamp}.pkl\")\n",
    "\n",
    "    print(\"-\" * 80)"
   ],
   "id": "b8799f07608a653c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 0 --- Title: Use of calcium-containing bioactive desensitizers in dental bleaching, id: https://openalex.org/W4318387462\n",
      "Top 3 topics: [Topic(id=11958,name=Dental Erosion and Tooth Whitening), Topic(id=12145,name=Dental Anxiety and Anesthetic Management in Dentistry), Topic(id=10368,name=Global Oral Health and Dental Caries)]\n",
      "Retrieved and inserted 2000 publications.\n",
      "Rebuilding BM25 index...\n",
      "Rebuilt BM25 index.\n",
      "Inserted 58 referenced works.\n",
      "Found all referenced works in the database? True\n",
      "Getting top 100 publications using SearchType.HYBRID search. Reranking enabled: False\n",
      "Top 5 results: \n",
      "\tEffect of an experimental desensitizing gel on bleaching-induced tooth sensitivity after in-office bleaching—a double-blind, randomized controlled trial\n",
      "\tA clinical, randomized, controlled study on the use of desensitizing agents during tooth bleaching\n",
      "\tNovel treatment of in‐office tooth bleaching sensitivity: A randomized, placebo‐controlled clinical study\n",
      "\tAssessment of the effect of experimental bleaching agent with nano‐bioactive material on postoperative sensitivity: A randomized, triple blind clinical trial\n",
      "\tAt-home Bleaching with a Novel Carbamide Peroxide Polymeric Nanoparticle Gel: A Multicenter Randomized Controlled Trial\n",
      "Reranking...\n",
      "Total compare: 112, Total prompt tokens: 180306, Total completion tokens: 336\n",
      "Reranked results: \n",
      "\tEffectiveness of nano-calcium phosphate paste on sensitivity during and after bleaching: a randomized clinical trial\n",
      "\tEffect of an experimental desensitizing gel on bleaching-induced tooth sensitivity after in-office bleaching—a double-blind, randomized controlled trial\n",
      "\tNovel treatment of in‐office tooth bleaching sensitivity: A randomized, placebo‐controlled clinical study\n",
      "\tThe use of desensitizing agents during in-office bleaching might not decrease tooth bleaching sensitivity: a randomized clinical trial.\n",
      "\tEvaluation of post-bleaching hypersensitivity using desensitizing agent before and /or after in-office bleaching: A randomized clinical trial.\n",
      "--------------------------------------------------------------------------------\n",
      "Run: 1 --- Title: PALEOCEANOGRAPHY, PHYSICAL AND CHEMICAL PROXIES | Carbon Cycle Proxies (δ11B, δ13Ccalcite, δ13Corganic, Shell Weights, B/Ca, U/Ca, Zn/Ca, Ba/Ca), id: https://openalex.org/W206144723\n",
      "Top 3 topics: [Topic(id=10109,name=Paleoredox and Paleoproductivity Proxies), Topic(id=12806,name=Impact of Ocean Acidification on Marine Ecosystems), Topic(id=10032,name=Marine Biogeochemistry and Ecosystem Dynamics)]\n",
      "Retrieved and inserted 2000 publications.\n",
      "Rebuilding BM25 index...\n",
      "Rebuilt BM25 index.\n",
      "Inserted 36 referenced works.\n",
      "Found all referenced works in the database? True\n",
      "Getting top 100 publications using SearchType.HYBRID search. Reranking enabled: False\n",
      "Top 5 results: \n",
      "\tSurface ocean pH response to variations in pCO2 through two full glacial cycles\n",
      "\tEffect of seawater carbonate concentration on foraminiferal carbon and oxygen isotopes\n",
      "\tB/Ca in planktonic foraminifera as a proxy for surface seawater pH\n",
      "\tMagnesium (Mg/Ca, δ<sup>26</sup>Mg), boron (B/Ca, δ<sup>11</sup>B), and calcium ([Ca<sup>2+</sup>]) geochemistry of Arctica islandica and Crassostrea virginica extrapallial fluid and shell under ocean acidification\n",
      "\tSeawater pH, pCO2 and [CO2−3] variations in the Caribbean Sea over the last 130 kyr: A boron isotope and B/Ca study of planktic foraminifera\n",
      "Reranking...\n",
      "Total compare: 113, Total prompt tokens: 147499, Total completion tokens: 339\n",
      "Reranked results: \n",
      "\tB/Ca in planktonic foraminifera as a proxy for surface seawater pH\n",
      "\tSurface ocean pH response to variations in pCO2 through two full glacial cycles\n",
      "\tForaminiferal Calcification Response to Glacial-Interglacial Changes in Atmospheric CO <sub>2</sub>\n",
      "\tEffect of seawater carbonate concentration on foraminiferal carbon and oxygen isotopes\n",
      "\tMagnesium (Mg/Ca, δ<sup>26</sup>Mg), boron (B/Ca, δ<sup>11</sup>B), and calcium ([Ca<sup>2+</sup>]) geochemistry of Arctica islandica and Crassostrea virginica extrapallial fluid and shell under ocean acidification\n",
      "--------------------------------------------------------------------------------\n",
      "Run: 2 --- Title: ¿Es posible la optimización del desarrollo de la identidad en la adolescencia? Intervenciones más allá del autoconcepto, id: https://openalex.org/W1600088747\n",
      "Top 3 topics: [Topic(id=11983,name=Development of Narrative Identity in Emerging Adulthood), Topic(id=12711,name=Intercultural Education in Europe and Spain), Topic(id=12175,name=The Dialogical Self: Personal and Cultural Positioning)]\n",
      "Retrieved and inserted 2000 publications.\n",
      "Rebuilding BM25 index...\n",
      "Rebuilt BM25 index.\n",
      "Inserted 23 referenced works.\n",
      "Found all referenced works in the database? True\n",
      "Getting top 100 publications using SearchType.HYBRID search. Reranking enabled: False\n",
      "Top 5 results: \n",
      "\tA Comparison of Two Approaches for Facilitating Identity Exploration Processes in Emerging Adults\n",
      "\tLa intervención educativa como medio de formación profesional docente\n",
      "\tPromoting Identity Development in Marginalized Youth\n",
      "\tPsicoeducación y la andragogía: un enfoque reflexivo en la educación de adultos\n",
      "\tKnowing Me, Knowing You: Changes in Parental Representations Among Established Adults Going Through Progressive Identity Development\n",
      "Reranking...\n",
      "Total compare: 127, Total prompt tokens: 188498, Total completion tokens: 381\n",
      "Reranked results: \n",
      "\tIn Search of Mechanisms of Change in Identity Development: Integrating the Constructivist and Discovery Perspectives on Identity\n",
      "\tA Comparison of Two Approaches for Facilitating Identity Exploration Processes in Emerging Adults\n",
      "\tThe Evolution of Eriksonian and, Neo-Eriksonian Identity Theory and Research: A Review and Integration\n",
      "\tDiscussions on Ego Identity\n",
      "\tPromoting Adolescents’ Heritage Cultural Identity Development: Exploring the Role of Autonomy and Relatedness Satisfaction in School-Based Interventions\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
